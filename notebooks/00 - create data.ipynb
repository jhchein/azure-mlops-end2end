{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes Dataset Preparation Notebook\n",
        "\n",
        "This notebook serves as the first step in a series of notebooks aimed at analyzing a diabetes dataset. In this notebook, we'll focus on data acquisition, sampling, and preparation for model training in subsequent steps. The key processes include:\n",
        "\n",
        "1. **Environment Setup**:\n",
        "    - Importing necessary libraries.\n",
        "    - Establishing Azure ML client connection.\n",
        "\n",
        "2. **Data Acquisition**:\n",
        "    - Defining the data paths.\n",
        "    - Loading data into `MLTable` objects using `mltable.from_delimited_files()`.\n",
        "\n",
        "3. **Data Sampling**:\n",
        "    - Random sampling of data using `mltable` functionality.\n",
        "    - Converting sampled data to Pandas DataFrame for further analysis.\n",
        "\n",
        "4. **Data Saving and Versioning**:\n",
        "    - Saving the data to disk in MLTable format.\n",
        "    - Creating or updating the data asset in Azure ML with versioning.\n",
        "\n",
        "5. **Data Splitting**:\n",
        "    - Splitting the data into training and testing sets using `train_test_split`.\n",
        "    - Exploring the split data (optional).\n",
        "\n",
        "6. **Data Persistence**:\n",
        "    - Saving the split data to disk for future use.\n",
        "\n",
        "This notebook sets the stage for the following notebooks in this series:\n",
        "- **Train and Deploy a Model**: Building, training, and deploying a machine learning model.\n",
        "- **Invoke a Real-Time Endpoint**: Making real-time predictions using the deployed model.\n",
        "- **Create Synthetic Data**: Generating synthetic data for further analysis.\n",
        "- **Explore Collected Data from Production**: Analyzing data collected from the production environment to gain insights and improve the model.\n",
        "\n",
        "**Note**: This notebook assumes familiarity with Azure ML, Scikit-learn, and the `mltable` library for tabular data manipulation and analysis. For more detailed information on `mltable`, please refer to the [official documentation](link_to_mltable_documentation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218725674
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# !pip install mltable scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218727685
        }
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import mltable  # Library for working with tabular data\n",
        "import os  # Operating system interfaces\n",
        "import time  # Time access and conversions\n",
        "from azure.ai.ml import MLClient  # Azure ML Client for managing ML assets\n",
        "from azure.ai.ml.entities import Data  # Data entity class for Azure ML\n",
        "from azure.ai.ml.constants import AssetTypes  # Constants for Azure ML asset types\n",
        "from azure.identity import DefaultAzureCredential  # Credential class for Azure authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "In this section, we'll load the diabetes dataset from two distinct files using `mltable.from_delimited_files()`. The paths variable holds dictionaries that specify the URLs of these CSV files. Allowed keys are `file`, `folder`, and `pattern`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218729085
        }
      },
      "outputs": [],
      "source": [
        "# Define the file paths\n",
        "paths = [\n",
        "    {\n",
        "        \"pattern\": \"https://raw.githubusercontent.com/MicrosoftLearning/mslearn-dp100/main/data/diabetes.csv\"\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"https://raw.githubusercontent.com/MicrosoftLearning/mslearn-dp100/main/data/diabetes2.csv\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"folder\": \"https://raw.githubusercontent.com/MicrosoftLearning/mslearn-dp100/main/data/\"\n",
        "    # }\n",
        "]\n",
        "\n",
        "# Create an MLTable object\n",
        "tbl = mltable.from_delimited_files(paths)\n",
        "\n",
        "tbl.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `mltable.from_delimited_files()` function simplifies the loading process by auto-detecting the delimiter and establishing appropriate column names and data types, encapsulating the data within an MLTable object for easy manipulation and analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Data\n",
        "For initial exploration and model testing, we'll draw a random sample from the loaded data. The `tbl.take_random_sample()` method facilitates this, where `probability` sets the row selection chance, and `seed` ensures reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218729517
        }
      },
      "outputs": [],
      "source": [
        "# Take a random sample of the table\n",
        "tbl_sample = tbl.take_random_sample(probability=0.001, seed=735)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = tbl_sample.to_pandas_dataframe()\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218729834
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "median_patient_id = str(int(df[\"PatientID\"].median()))\n",
        "median_patient_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218730145
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# tbl_sample.random_split(percent=.5)\n",
        "tbl_dev = tbl.filter(f'PatientID <= {median_patient_id}')\n",
        "tbl_prod = tbl.filter(f'PatientID > {median_patient_id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Enriching Production Data with Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697221160813
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "tbl_prod_features, tbl_prod_labels = tbl_prod.drop_columns([\"Diabetic\", \"PatientID\"]), tbl_prod.keep_columns([\"Diabetic\"])\n",
        "tbl_prod_features.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697221328832
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "df_prod_features = tbl_prod_features.to_pandas_dataframe()\n",
        "\n",
        "# Get today's date\n",
        "today = datetime.today()\n",
        "\n",
        "# Generate random number of days between -30 and 30\n",
        "random_days = np.random.randint(-30, 30, size=len(df_prod_features))\n",
        "\n",
        "# Create date column with random dates\n",
        "df_prod_features['date'] = [(today + timedelta(days=int(d))).isoformat() + 'Z' for d in random_days]\n",
        "\n",
        "df_prod_features.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Saving and Versioning\n",
        "\n",
        "Post sampling, we persist the `MLTable` object to disk, and define a version for the data asset based on the current UTC time. This versioning can be invaluable for traceability and managing data versions in machine learning workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218730355
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set the version number of the data asset to the current UTC time\n",
        "VERSION = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218730674
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# delete model directory if it exists\n",
        "import shutil\n",
        "if os.path.exists(\"data\"):\n",
        "    shutil.rmtree(\"data\")\n",
        "\n",
        "os.makedirs(\"data\")\n",
        "os.makedirs(\"data/train\")\n",
        "os.makedirs(\"data/test\")\n",
        "os.makedirs(\"data/eval\")\n",
        "os.makedirs(\"data/prod\")\n",
        "os.makedirs(\"data/prod/features\")\n",
        "os.makedirs(\"data/prod/labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218731463
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the table to disk in MLTable format\n",
        "tbl_prod_labels.save(\"./data/prod/labels\")\n",
        "tbl_dev.save(\"./data/dev\")\n",
        "\n",
        "print(os.listdir(\"./data/prod/features\"))\n",
        "\n",
        "with open(\"./data/prod/labels/MLTable\") as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connect to the AzureML workspace and define the data asset. The `create_or_update` method is employed to either create a new data asset or update an existing one in AzureML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697221473785
        }
      },
      "outputs": [],
      "source": [
        "df_prod_features.to_parquet(\"./data/prod/features/data.parquet\")\n",
        "\n",
        "# Connect to the AzureML workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "# Define the data asset\n",
        "my_data = Data(\n",
        "    path=\"./data/prod/features\",\n",
        "    type=AssetTypes.URI_FOLDER,\n",
        "    description=\"Example data from the diabetes dataset (Production)\",\n",
        "    name=\"diabetes-urifolder-production\",\n",
        "    version=VERSION,\n",
        ")\n",
        "\n",
        "# Create or update the data asset in AzureML\n",
        "ml_client.data.create_or_update(my_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218735913
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Connect to the AzureML workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "# Define the data asset\n",
        "my_data = Data(\n",
        "    path=\"./data/prod/labels\",\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    description=\"Example data from the diabetes dataset (Production Labels)\",\n",
        "    name=\"diabetes-mltable-production-labels\",\n",
        "    version=VERSION,\n",
        ")\n",
        "\n",
        "# Create or update the data asset in AzureML\n",
        "ml_client.data.create_or_update(my_data)\n",
        "\n",
        "\n",
        "# Connect to the AzureML workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "# Define the data asset\n",
        "my_data = Data(\n",
        "    path=\"./data/dev/\",\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    description=\"Example data from the diabetes dataset (Train, Test, Eval)\",\n",
        "    name=\"diabetes-mltable-dev\",\n",
        "    version=VERSION,\n",
        ")\n",
        "\n",
        "# Create or update the data asset in AzureML\n",
        "ml_client.data.create_or_update(my_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Data\n",
        "The traditional approach would be to utilize `train_test_split` to segregate the data into training and testing sets, facilitating model validation on unseen data. \n",
        "\n",
        "While the above approach works well, we'll use the native MLTable functionality, which makes it quite easy to split data into train, test, and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218736460
        }
      },
      "outputs": [],
      "source": [
        "tbl_train, tbl_test_eval = tbl_dev.random_split(percent=.7)\n",
        "tbl_test, tbl_eval = tbl_test_eval.random_split(percent=.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Once we use sample, we need to store the file locally, create a new MLTable pointing to that file, and register the Data Asset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218740529
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "tables = {\"train\": tbl_train, \"test\": tbl_test, \"eval\": tbl_eval}\n",
        "\n",
        "for stage in tables.keys():\n",
        "    print(f\"Registering stage: {stage}\")\n",
        "    path = f\"./data/{stage}/\"\n",
        "    parquet_path = path + f\"{stage}.parquet\" \n",
        "    print(parquet_path)\n",
        "\n",
        "    table = tables.get(stage)\n",
        "\n",
        "    df = table.to_pandas_dataframe()\n",
        "\n",
        "    # Save files locally\n",
        "    df.to_parquet(parquet_path)\n",
        "\n",
        "    # Create new MLTable file, pointing towards the local files.\n",
        "    tbl = mltable.from_parquet_files(paths=[{\"file\": parquet_path}])\n",
        "    tbl.save(path)\n",
        "    \n",
        "    # Define Data Asset\n",
        "    my_data = Data(\n",
        "        path=path,\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        description=f\"Example data from the diabetes dataset ({stage})\",\n",
        "        name=f\"diabetes-mltable-{stage}\",\n",
        "        version=VERSION,\n",
        "    )\n",
        "\n",
        "    # Create or update the data asset in AzureML\n",
        "    # try:\n",
        "    ml_client.data.create_or_update(my_data)\n",
        "    # except TypeError as e:\n",
        "    #     print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sectioning enables a streamlined workflow from data loading to splitting, ensuring the data is readily accessible for subsequent analysis and modeling in following notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next step\n",
        "\n",
        "We've created some data assets, that lay the foundation of this accelerator. Proceed with the next notebook to understand how to train and deploy Models in AzureML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
