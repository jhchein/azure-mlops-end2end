{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train and deploy a model\n",
        "\n",
        "We'll now delve into how to \"manually\" (leveraging the SDK and this Jupyter notebook) train a model in AzureML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a model\n",
        "\n",
        "Initiate a connection to the Azure ML workspace and set up MLflow for tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218927660
        }
      },
      "outputs": [],
      "source": [
        "## Train a model\n",
        "\n",
        "# Handle to the workspace\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "import mlflow\n",
        "\n",
        "ml_client = MLClient.from_config(\n",
        "    DefaultAzureCredential()\n",
        ")\n",
        "\n",
        "# Gather MLflow URI information from workspace\n",
        "azureml_mlflow_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri\n",
        "mlflow.set_tracking_uri(azureml_mlflow_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries and set up the experiment in MLflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218929573
        }
      },
      "outputs": [],
      "source": [
        "# Import python packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "experiment_name = \"Monitoring-Models-Experiment\"\n",
        "mlflow.set_experiment(experiment_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset, convert it to a Pandas DataFrame, and prepare the directory for model saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218932478
        }
      },
      "outputs": [],
      "source": [
        "import mltable\n",
        "\n",
        "# iterate over all versions of the data asset\n",
        "\n",
        "data_asset = ml_client.data.get(\"diabetes-mltable-dev\", label=\"latest\") # You can also specify a specific version\n",
        "\n",
        "tbl = mltable.load(data_asset.path)\n",
        "\n",
        "df = tbl.to_pandas_dataframe()\n",
        "df\n",
        "\n",
        "model_path = \"./models/monitoring\"\n",
        "os.makedirs(model_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start logging the training process in MLflow, train a Decision Tree model, and log the model performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218934111
        }
      },
      "outputs": [],
      "source": [
        "# delete model directory if it exists\n",
        "import shutil\n",
        "if os.path.exists(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# Enable autologging (optional)\n",
        "# mlflow.sklearn.autolog()\n",
        "\n",
        "diabetes = df\n",
        "\n",
        "feature_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
        "\n",
        "# Breaking up data into input/target features\n",
        "X, y = diabetes[feature_cols].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Breaking data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Training a model:\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance and logging them\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "mlflow.log_metric('Accuracy', float(acc))\n",
        "\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "mlflow.log_metric('AUC', float(auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Infer the model signature, register the model to the workspace, and save the model to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697218945451
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from mlflow.models import infer_signature\n",
        "\n",
        "signature = infer_signature(X_test, y_hat)\n",
        "\n",
        "model_name = \"monitoring-diabetes\"\n",
        "\n",
        "# Registering the model to the workspace\n",
        "print(\"Registering the model via MLFlow\")\n",
        "mlflow.sklearn.log_model(\n",
        "    sk_model=model,\n",
        "    registered_model_name=model_name,\n",
        "    artifact_path=\"model\",\n",
        "    signature=signature\n",
        ")\n",
        "\n",
        "# Saving the model to a file\n",
        "mlflow.sklearn.save_model(\n",
        "    sk_model=model, \n",
        "    path=model_path,\n",
        "    signature=signature\n",
        ")\n",
        "\n",
        "# Stop logging\n",
        "mlflow.end_run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now also want to save the training data for model monitoring. We therefore use the same Version as the model version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import mltable\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(X_train, columns=feature_cols).to_csv(\"./data/train/X_train.csv\", index=False)\n",
        "\n",
        "tbl_train = mltable.from_delimited_files(paths=[{\"file\": \"./data/train/X_train.csv\"}])\n",
        "tbl_train.save(\"./data/train\")\n",
        "\n",
        "# Define the data asset\n",
        "training_data = Data(\n",
        "    path=\"./data/train/\",\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    description=f\"Training data for model {model_name}\",\n",
        "    name=\"diabetes-mltable-train\",\n",
        "    version=time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime()),\n",
        ")\n",
        "\n",
        "# Create or update the data asset in AzureML\n",
        "ml_client.data.create_or_update(training_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy a Model\n",
        "\n",
        "After training and registering the model, it's time to deploy it. The easiest and most explaining way is to use the AzureML Studio UI (you can of course also leverage the SDK, using the commented code). \n",
        "\n",
        "Deployment is really easy with **MLFlow models**, because we know the signature and dependencies of the model and therefore don't need to define a scoring script and environment.\n",
        "\n",
        "Under `Models` select the recently trained model `monitoring-diabetes` and select `Deploy` as a `Real-time endpoint`.\n",
        "\n",
        "- This action will create a new `Deployment` on a new or existing `Endpoint` (You can deploy multiple models behind one endpoint).\n",
        "- Keep the instances at 3 instances, so that we can add autoscaling later.\n",
        "- Make sure to enable data collection (still in preview as of Nov '23).\n",
        "\n",
        "> Note: The deployment will take **ten to fifteen minutes**, make sure to **grab a cup of coffee**. &#9749;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Deploy a Model\n",
        "\n",
        "# # Name the model you registered earlier in the training script\n",
        "# registered_model_name = \"monitoring-diabetes\"\n",
        "\n",
        "# # Let's pick the latest version of the model\n",
        "# latest_model_version = max(\n",
        "#     [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
        "# )\n",
        "\n",
        "# from azure.ai.ml.entities import (\n",
        "#     ManagedOnlineEndpoint,\n",
        "#     ManagedOnlineDeployment,\n",
        "#     Model,\n",
        "# )\n",
        "\n",
        "# import datetime\n",
        "\n",
        "# # Creating a unique online endpoint name with current datetime to avoid conflicts\n",
        "# online_endpoint_name = \"monitor-diabetes-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# # Create an online endpoint\n",
        "# endpoint = ManagedOnlineEndpoint(\n",
        "#   name=online_endpoint_name,\n",
        "#   description=\"This is a diabetes classifier online endpoint\",\n",
        "#   auth_mode=\"key\",\n",
        "#   tags={\n",
        "#       \"training_dataset\": \"diabetes-data\",\n",
        "#       \"model_type\": \"sklearn.DecisionTreeClassifier\",\n",
        "#       \"purpose\": \"demonstration\"\n",
        "#   },\n",
        "# )\n",
        "\n",
        "# endpoint_lro = ml_client.begin_create_or_update(endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# while endpoint_lro.status() in [\"InProgress\", \"Updating\"]:\n",
        "#     print(\"Endpoint creation in progress...\", end='', flush=True)\n",
        "#     time.sleep(5)\n",
        "# print(\"Endpoint status: \", endpoint_lro.status())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# from azure.ai.ml.entities import DataCollector\n",
        "\n",
        "# model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "# notebook_deployment = ManagedOnlineDeployment(\n",
        "#     name=\"notebook-deployment-1\",\n",
        "#     endpoint_name=online_endpoint_name,\n",
        "#     model=model,\n",
        "#     instance_type=\"Standard_F4s_v2\",\n",
        "#     instance_count=1,\n",
        "#     app_insights_enabled=True,\n",
        "#     # data_collector=DataCollector(\n",
        "#     #     collections={\"collections\": ...},\n",
        "#     # )\n",
        "# )\n",
        "\n",
        "# # try:\n",
        "# deployment_lro = ml_client.online_deployments.begin_create_or_update(notebook_deployment)\n",
        "# print(\"Creating notebook deployment on endpoint\")\n",
        "# # except Exception as e:\n",
        "# #     print(e)\n",
        "# #     print(\"Waiting 3 Minutes...\")\n",
        "# #     time.sleep(180)\n",
        "# #     deployment_lro = ml_client.online_deployments.begin_create_or_update(notebook_deployment)\n",
        "# #     print(\"Creating notebook deployment on endpoint\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# while deployment_lro.status() in [\"InProgress\", \"Updating\"]:\n",
        "#     print(\"\\rDeployment creation in progress...\", end='', flush=True)\n",
        "#     time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the endpoint\n",
        "\n",
        "### 1) Test inference\n",
        "\n",
        "Once the deploymet is complete go to `Endpoints`, select the newly deployed endpoint, and then select `Test`. \n",
        "\n",
        "Inference test data should be pre-populated. If for some reason it is not, use this JSON Data\n",
        "```JSON\n",
        "[11.0, 97.0, 89.0, 11.0, 23.0, 46.47006691, 1.476670289, 39.0],\n",
        "[3.0, 108.0, 63.0, 45.0, 297.0, 49.37516891, 0.100979095, 46.0],\n",
        "[9, 103, 78, 25, 304, 29.58219193, 1.282869847, 43]\n",
        "```\n",
        "or `notebooks/test.json` for scoring.\n",
        "\n",
        "### 2) Check the logs\n",
        "\n",
        "Now check the `Logs` (right next to `Test`). \n",
        "These logs provide valuable insights into the inference, especially when debugging model deployments.\n",
        "\n",
        "> Make sure to select the correct Deployment when you deployed multiple models behind the endpoint.\n",
        "\n",
        "### 3) Configure auto scaling\n",
        "\n",
        "\n",
        "Lastly, there are a few options to auto scale our endpoint. \n",
        "\n",
        "First let's go back to `Details` and search for the correct Deployment (remember you can have multiple `Deployments` per `Endpoint`). \n",
        "\n",
        "1) Right of the Deployment name, you'll find a `Pen Icon` to edit the Deployment. Check Deployment scale type **`Target Utilization`** (e.g. 70%). This will enable us to define auto-scaling based on the desired CPU load.\n",
        "\n",
        "2) If you need more control over your autoscaling, go back to the Deployment and check `Configure auto scaling`. You are forwarded to the Model Deployment in the Azure Portal, where you can configure `Custom autoscale`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Collection\n",
        "\n",
        "Now after having scored some samples (from running the Tests before), head to `Data` within the Studio. \n",
        "\n",
        "You'll now find a few more datasets such as `<endpointname>-inputs` (features) and `<endpointname>-outputs` (predictions). Explore the data that is automatically collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitoring the Endpoint\n",
        "\n",
        "Finally go back to the `Endpoint`, scroll to the bottom and select `View metrics`. This will lead you to the `Azure Application Insights` instance that logs your workspace. \n",
        "\n",
        "Explore the metrics logged in your `Application Insights`. \n",
        "\n",
        "Be aware, some metrics and logs are only collected after you enabled `Diagnostic settings`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitoring: Dataset and Model Drift\n",
        "\n",
        "Azure ML recently added the preview of Model and Data monitoring. We can now leverage this powerful feature via `Monitoring` in the AzureML Studio.\n",
        "\n",
        "Add a new monitor via `+ Add` and follow the wizard. \n",
        "- Select the model and deployment we just trained and created.\n",
        "- Under `Configure data assets` add the training data `diabetes-mltable-train` we registered a bit earlier.\n",
        "- Under `Select monitoring signals` make sure to hit edit one of the signal by selecting `Diabetic` as the target column.\n",
        "\n",
        "Create the monitor, grab another coffee, and chat about your learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Monitoring Drift: Bring your own data\n",
        "\n",
        "If you are keen on exploring **how to bring your own production data** (that is not collected from an online endpoint) now is your time to understand how that works.\n",
        "\n",
        "Remember, we registered a `diabetes-urifolder-production` dataset in the first notebook. This repository also contains a `preprocess component` (in the `components/preprocess_production_data` directory). \n",
        "\n",
        "You can use both to investigate how you could bring your own data, and (if you know how to register components) set up a monitor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "In the next notebook we'll learn more about Data Drift. Continue with the next notebook to **Create Synthentic Data** and then gather inference data that triggers data drift detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
